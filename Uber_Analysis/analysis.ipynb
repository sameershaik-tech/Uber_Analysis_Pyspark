{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/homebrew/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/homebrew/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('Uber').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n",
      " Date            | 10-Sep-12 \n",
      " Time (Local)    | 7         \n",
      " Eyeballs        | 5         \n",
      " Zeroes          | 0         \n",
      " Completed Trips | 2         \n",
      " Requests        | 2         \n",
      " Unique Drivers  | 9         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "df=spark.read.csv(\"dataset.csv\",header=True,inferSchema=True)\n",
    "trimmed_column_names = [col_name.strip() for col_name in df.columns]\n",
    "\n",
    "# Create a dictionary to map the current column names to the trimmed column names\n",
    "column_rename_mapping = dict(zip(df.columns, trimmed_column_names))\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "for old_col_name, new_col_name in column_rename_mapping.items():\n",
    "    df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "# Show the DataF\n",
    "\n",
    "df.show(1,vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime ,timedelta\n",
    "import random\n",
    "\n",
    "# Function to generate random dates\n",
    "def generate_random_date(start_date, end_date):\n",
    "    start_datetime = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = end_datetime - start_datetime\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    random_date = start_datetime + timedelta(days=random_days)\n",
    "    return random_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the range for random dates (you can adjust this range as needed)\n",
    "start_date='2023-01-01'\n",
    "end_date='2024-06-01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "\n",
    "# Define the generate_random_time function before it's used in the UDF\n",
    "def generate_random_time():\n",
    "    return random.randint(1, 86400) \n",
    "\n",
    "# Register the UDF\n",
    "random_time_udf = udf(generate_random_time, IntegerType())\n",
    "\n",
    "# Apply the UDF to fill null values in the \"Time (Local)\" column\n",
    "df = df.withColumn(\"Time (Local)\", when(col(\"Time (Local)\").isNull(), random_time_udf()).otherwise(col(\"Time (Local)\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+------+---------------+--------+--------------+\n",
      "|      Date|Time (Local)|Eyeballs|Zeroes|Completed Trips|Requests|Unique Drivers|\n",
      "+----------+------------+--------+------+---------------+--------+--------------+\n",
      "| 10-Sep-12|           7|       5|     0|              2|       2|             9|\n",
      "|2023-08-31|           8|       6|     0|              2|       2|            14|\n",
      "|2024-04-14|           9|       8|     3|              0|       0|            14|\n",
      "|2023-12-31|          10|       9|     2|              0|       1|            14|\n",
      "|2023-01-05|          11|      11|     1|              4|       4|            11|\n",
      "|2023-03-11|          12|      12|     0|              2|       2|            11|\n",
      "|2023-06-02|          13|       9|     1|              0|       0|             9|\n",
      "|2023-09-23|          14|      12|     1|              0|       0|             9|\n",
      "|2024-03-17|          15|      11|     2|              1|       2|             7|\n",
      "|2023-10-17|          16|      11|     2|              3|       4|             6|\n",
      "|2023-05-29|          17|      12|     2|              3|       4|             4|\n",
      "|2024-02-28|          18|      11|     1|              3|       4|             7|\n",
      "|2023-04-15|          19|      13|     2|              2|       3|             7|\n",
      "|2023-04-15|          20|      11|     1|              0|       0|             5|\n",
      "|2023-09-07|          21|      11|     0|              1|       1|             3|\n",
      "|2023-07-05|          22|      16|     3|              0|       2|             4|\n",
      "|2024-03-18|          23|      21|     5|              3|       3|             4|\n",
      "| 11-Sep-12|           0|       9|     3|              1|       1|             3|\n",
      "|2023-10-08|           1|       3|     2|              0|       1|             3|\n",
      "|2023-09-06|           2|       1|     1|              0|       0|             1|\n",
      "+----------+------------+--------+------+---------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,when,col\n",
    "\n",
    "\n",
    "# Replace null values in the \"Date\" column with random dates\n",
    "random_date_udf = udf(lambda: generate_random_date(start_date,end_date))\n",
    "df = df.withColumn(\"Date\", when(col(\"Date\").isNull(), random_date_udf()).otherwise(col(\"Date\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Trip Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      Date|Total Completed Trips |\n",
      "+----------+----------------------+\n",
      "|      NULL|                   104|\n",
      "|2023-01-01|                     4|\n",
      "|2023-01-02|                     1|\n",
      "|2023-01-03|                     5|\n",
      "|2023-01-06|                     2|\n",
      "|2023-01-10|                     0|\n",
      "|2023-01-11|                    13|\n",
      "|2023-01-13|                     0|\n",
      "|2023-01-15|                     2|\n",
      "|2023-01-16|                     1|\n",
      "|2023-01-17|                    12|\n",
      "|2023-01-20|                     1|\n",
      "|2023-01-21|                     5|\n",
      "|2023-01-25|                     5|\n",
      "|2023-01-26|                    25|\n",
      "|2023-01-27|                     1|\n",
      "|2023-01-30|                     2|\n",
      "|2023-02-04|                     0|\n",
      "|2023-02-05|                     0|\n",
      "|2023-02-07|                     0|\n",
      "+----------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,to_date\n",
    "# Convert Date column to DateType\n",
    "df = df.withColumn(\"Date\",to_date(\"Date\"))\n",
    "\n",
    "daily_trip_trends = df.groupBy(\"Date\").agg(sum(\"Completed Trips\").alias(\"Total Completed Trips \")) \\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "daily_trip_trends.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Hours Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "+----+---------------------+\n",
      "|Hour|Total Completed Trips|\n",
      "+----+---------------------+\n",
      "|   5|                 1365|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour,from_unixtime\n",
    "df = df.withColumn(\"Time (Local)\", from_unixtime(\"Time (Local)\"))\n",
    "\n",
    "null_count = df.filter(df[\"Time (Local)\"].isNull()).count()\n",
    "\n",
    "print(null_count)\n",
    "\n",
    "peak_hours_analysis = df \\\n",
    "    .withColumn(\"Hour\", hour(\"Time (Local)\")) \\\n",
    "    .groupBy(\"Hour\") \\\n",
    "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "peak_hours_analysis.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekday vs. Weekend Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|Weekend|sum(Completed Trips)|\n",
      "+-------+--------------------+\n",
      "|Weekday|                1002|\n",
      "|Weekend|                 363|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "weekday_vs_weekend = df.withColumn(\"DayOfWeek\",dayofweek(\"Date\")) \\\n",
    "    .withColumn(\"Weekend\",when(col(\"DayOfWeek\").isin(1,7),\"Weekend\").otherwise(\"Weekday\")) \\\n",
    "    .groupBy(\"Weekend\").agg(sum(\"Completed Trips\")).alias(\"Total Completed Trips\")\n",
    "\n",
    "\n",
    "\n",
    "weekday_vs_weekend.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Fulfillment Rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|Request Fulfillment Rate|\n",
      "+------------------------+\n",
      "|      0.7346609257265877|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "request_fulfillment_rate = df.agg((sum(\"Completed Trips\")/sum(\"Requests\")).alias(\"Request Fulfillment Rate\"))\n",
    "\n",
    "request_fulfillment_rate.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Utilization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+--------------+----------------+\n",
      "|      Date|Total Completed Trips|Unique Drivers|Utilization Rate|\n",
      "+----------+---------------------+--------------+----------------+\n",
      "|2024-05-30|                    9|             1|             9.0|\n",
      "|2023-05-22|                    2|             1|             2.0|\n",
      "|2023-02-25|                    7|             1|             7.0|\n",
      "|2024-04-20|                    0|             1|             0.0|\n",
      "|2024-01-11|                    0|             1|             0.0|\n",
      "|2023-09-27|                    0|             1|             0.0|\n",
      "|2023-06-24|                    0|             2|             0.0|\n",
      "|2023-10-12|                    0|             1|             0.0|\n",
      "|2024-02-16|                    0|             1|             0.0|\n",
      "|2023-05-16|                    2|             1|             2.0|\n",
      "|2023-06-01|                   14|             1|            14.0|\n",
      "|2023-02-16|                    0|             1|             0.0|\n",
      "|2024-04-06|                    9|             1|             9.0|\n",
      "|2023-07-31|                    4|             1|             4.0|\n",
      "|2023-08-06|                    0|             1|             0.0|\n",
      "|2024-05-05|                    2|             1|             2.0|\n",
      "|2024-03-12|                    8|             1|             8.0|\n",
      "|2024-04-15|                   17|             1|            17.0|\n",
      "|2023-12-11|                    2|             1|             2.0|\n",
      "|2023-11-05|                    1|             1|             1.0|\n",
      "+----------+---------------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, sum\n",
    "\n",
    "# Calculate the total completed trips using sum aggregation\n",
    "total_completed_trips = df.groupBy(\"Date\").agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\"))\n",
    "\n",
    "# Calculate the utilization rate using the total completed trips and count of distinct unique drivers\n",
    "driver_utilization = total_completed_trips.join(\n",
    "    df.groupBy(\"Date\").agg(countDistinct(\"Unique Drivers\").alias(\"Unique Drivers\")),\n",
    "    \"Date\"\n",
    ").withColumn(\"Utilization Rate\", col(\"Total Completed Trips\") / col(\"Unique Drivers\"))\n",
    "\n",
    "driver_utilization.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eyeballs vs. Completed Trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+\n",
      "|Eyeballs|Total Completed Trips|\n",
      "+--------+---------------------+\n",
      "|      31|                   15|\n",
      "|      53|                   28|\n",
      "|      34|                   45|\n",
      "|      28|                   24|\n",
      "|      26|                   22|\n",
      "|      27|                    9|\n",
      "|      44|                   17|\n",
      "|      12|                    8|\n",
      "|      22|                   36|\n",
      "|      47|                   16|\n",
      "|       1|                    0|\n",
      "|      13|                   11|\n",
      "|       6|                    6|\n",
      "|      16|                   21|\n",
      "|       3|                    3|\n",
      "|      20|                   29|\n",
      "|      40|                   20|\n",
      "|      94|                   36|\n",
      "|       5|                    8|\n",
      "|      19|                   39|\n",
      "+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eyeballs_vs_completed = df.groupBy(\"Eyeballs\").agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\"))\n",
    "\n",
    "\n",
    "eyeballs_vs_completed.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroes Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero entries in 'Zeroes': 310\n"
     ]
    }
   ],
   "source": [
    "zeroes_analysis = df.filter(col(\"Zeroes\")!=0).count()\n",
    "\n",
    "print(\"Number of non-zero entries in 'Zeroes':\",zeroes_analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Patterns by Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|hour(Time (Local))|Total Requests|\n",
      "+------------------+--------------+\n",
      "|                 5|           336|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour,count\n",
    "\n",
    "request_patterns_by_time = df.groupBy(hour(\"Time (Local)\")).agg(count(\"Requests\").alias(\"Total Requests\")) \\\n",
    "                            .orderBy(\"Total Requests\",ascending=False)\n",
    "\n",
    "request_patterns_by_time.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random latitude and longitude values for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " Date            | NULL                \n",
      " Time (Local)    | 1970-01-01 05:30:07 \n",
      " Eyeballs        | 5                   \n",
      " Zeroes          | 0                   \n",
      " Completed Trips | 2                   \n",
      " Requests        | 2                   \n",
      " Unique Drivers  | 9                   \n",
      " Latitude        | -75.48375177280542  \n",
      " Longitude       | 38.4630733315166    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = df.withColumn(\"Latitude\", lit(random.uniform(-90, 90))) \\\n",
    "                     .withColumn(\"Longitude\", lit(random.uniform(-180, 180)))\n",
    "\n",
    "# Show the DataFrame with generated latitude and longitude values\n",
    "df.show(1,vertical=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+---------------------+\n",
      "|          Latitude|       Longitude|Total Completed Trips|\n",
      "+------------------+----------------+---------------------+\n",
      "|-75.48375177280542|38.4630733315166|                 1365|\n",
      "+------------------+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_analysis = df.groupBy(\"Latitude\",\"Longitude\").agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "                .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "geo_analysis.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analysis Over Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+--------------+--------------+\n",
      "|      Date|Total Completed Trips|Total Requests|Unique Drivers|\n",
      "+----------+---------------------+--------------+--------------+\n",
      "|      NULL|                  104|           144|             9|\n",
      "|2023-01-07|                    9|             9|             1|\n",
      "|2023-01-10|                    0|             2|             1|\n",
      "|2023-01-11|                    3|             5|             2|\n",
      "|2023-01-13|                    7|            10|             1|\n",
      "|2023-01-14|                    6|             7|             1|\n",
      "|2023-01-18|                    1|             2|             1|\n",
      "|2023-01-19|                    0|             0|             1|\n",
      "|2023-01-21|                    3|             4|             1|\n",
      "|2023-01-22|                    8|             9|             1|\n",
      "|2023-01-24|                    3|             3|             1|\n",
      "|2023-01-27|                    2|             4|             1|\n",
      "|2023-01-29|                    5|             7|             1|\n",
      "|2023-01-31|                   11|            12|             1|\n",
      "|2023-02-01|                    1|             2|             2|\n",
      "|2023-02-05|                    5|             5|             1|\n",
      "|2023-02-06|                    0|             1|             1|\n",
      "|2023-02-08|                    0|             0|             1|\n",
      "|2023-02-10|                    1|             4|             1|\n",
      "|2023-02-12|                   18|            26|             2|\n",
      "+----------+---------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trend_analysis_over_time = df.groupBy(\"Date\").agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\"),\n",
    "                                                  sum(\"Requests\").alias(\"Total Requests\"),\n",
    "                                                  countDistinct(\"Unique Drivers\").alias(\"Unique Drivers\")) \\\n",
    "                                                  .orderBy(\"Date\")\n",
    "\n",
    "\n",
    "\n",
    "trend_analysis_over_time.show()\n",
    "                                                  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
